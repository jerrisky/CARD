import os
import yaml

# 12 ä¸ª LDL æ•°æ®é›†åˆ—è¡¨
datasets = [
    "Flickr_LDL", "SBU_3DFE", "Scene", "Gene", "Movie", "RAF_ML", 
    "Ren_Cecps", "SJAFFE", "M2B", "SCUT_FBP5500", "Twitter_LDL", "SCUT_FBP"
]

# ä¸¥è°¨å¯¹é½åŸå§‹ CARD cifar10.yml çš„æ¨¡æ¿
# åŒæ—¶ä¿ç•™äº† LDL ä»»åŠ¡æ‰€éœ€çš„ MLP ç»“æ„ (simple/linear)
# ä¿®æ­£äº† validation_freq = 10 (å¯¹é½åŸå§‹)
# ä¿®æ­£äº† snapshot_freq = 1e9 (å¯¹é½åŸå§‹)
template = """data:
  dataset: "{dataset_name}"
  seed: 2000
  label_min_max: [0.001, 0.999]  # å¯¹é½åŸå§‹ï¼šä¿ç•™æ­¤å‚æ•°
  num_classes: 0                 # ä»£ç ä¼šè‡ªåŠ¨è¦†ç›–
  num_workers: 4
  dataroot: '../Data/feature/{dataset_name}' # è‡ªåŠ¨åŒ¹é…è·¯å¾„
  run_idx: 0                     # ä½ çš„æ–°å¢å‚æ•°

model:
  type: "simple"
  data_dim: 0                    # ä»£ç ä¼šè‡ªåŠ¨è¦†ç›–
  n_input_channels: 3            # å¯¹é½åŸå§‹ï¼šä¿ç•™ç»“æ„
  n_input_padding: 0             # å¯¹é½åŸå§‹ï¼šä¿ç•™ç»“æ„
  feature_dim: 512               # MLP å®½åº¦ (å¯æŒ‰éœ€ç»Ÿä¸€ä¿®æ”¹)
  hidden_dim: 512                # MLP å®½åº¦
  cat_x: True
  cat_y_pred: True
  arch: linear                   # LDL ä»»åŠ¡ç‰¹å®š
  var_type: fixedlarge
  ema_rate: 0.9999
  ema: True

diffusion:
  beta_schedule: linear
  beta_start: 0.0001
  beta_end: 0.02
  timesteps: 1000
  vis_step: 100
  num_figs: 10
  include_guidance: True
  apply_aux_cls: True
  trained_aux_cls_ckpt_path: ''
  trained_aux_cls_ckpt_name: ''
  aux_cls:
    arch: linear                 # LDL ä»»åŠ¡ç‰¹å®š
    pre_train: True
    joint_train: False
    n_pretrain_epochs: 100
    logging_interval: 10

training:
  batch_size: 128
  n_epochs: 5000                 
  warmup_epochs: 40
  add_t0_loss: False
  n_steps_req_grad: 100
  n_minibatches_add_ce: 20
  n_ce_epochs_warmup: 10
  n_ce_epochs_interval: 50
  n_sanity_check_epochs_freq: 500
  snapshot_freq: 1000000000      # å¯¹é½åŸå§‹ï¼šç¦ç”¨æŒ‰ Step ä¿å­˜
  logging_freq: 100              # å»ºè®®ï¼šåŸå§‹æ˜¯1200(é’ˆå¯¹å¤§å›¾é›†)ï¼ŒLDLæ•°æ®å°‘ï¼Œè®¾100æ›´åˆç†
  validation_freq: 10            # å¯¹é½åŸå§‹ï¼šæ¯ 10 ä¸ª Epoch éªŒè¯ä¸€æ¬¡
  image_folder: 'training_image_samples'

sampling:
  batch_size: 256
  sampling_size: 1000
  last_only: True
  image_folder: 'sampling_image_samples'

testing:
  batch_size: 256
  sampling_size: 1000
  last_only: True
  plot_freq: 200
  image_folder: 'testing_image_samples'
  n_samples: 100
  n_bins: 10
  compute_metric_all_steps: False
  metrics_t: 0
  ttest_alpha: 0.05
  trimmed_mean_range: [0.0, 100.0]
  PICP_range: [2.5, 97.5]
  make_plot: False
  squared_plot: False
  plot_true: False
  plot_gen: False
  fig_size: [8, 5]

optim:
  weight_decay: 0.000
  optimizer: "Adam"
  lr: 0.001
  beta1: 0.9
  amsgrad: False
  eps: 0.00000001
  grad_clip: 1.0
  lr_schedule: True
  min_lr: 0.0

aux_optim:
  weight_decay: 0.000
  optimizer: "Adam"
  lr: 0.001
  beta1: 0.9
  amsgrad: True
  eps: 0.00000001
  grad_clip: 1.0
"""

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
output_dir = "configs" # æˆ–è€… "config"ï¼Œæ ¹æ®ä½ çš„æ–‡ä»¶å¤¹åä¿®æ”¹
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

print(f"ğŸš€ Generating 12 aligned config files in '{output_dir}/'...")

for ds in datasets:
    # å¡«å…¥æ•°æ®é›†åç§°
    content = template.format(dataset_name=ds)
    
    file_path = os.path.join(output_dir, f"{ds}.yml")
    with open(file_path, "w", encoding='utf-8') as f:
        f.write(content)
    
    print(f"âœ… Generated: {file_path}")

print("\nğŸ‰ All 12 configs are strictly aligned with CARD-original settings.")